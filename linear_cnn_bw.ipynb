{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n",
      "Building Train Data loader for dataset:  Co3d(19800 pairs,split='train',seed=None,resolutions=[512x512],transform=Compose( ToTensor() Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))))\n",
      "Train dataset length:  19800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.__repr__ of AsymmetricCroCo3DStereo(\n",
       "  (patch_embed): PatchEmbedDust3R(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (mask_generator): RandomMask()\n",
       "  (rope): RoPE2D()\n",
       "  (enc_blocks): ModuleList(\n",
       "    (0-23): 24 x Block(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): RoPE2D()\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (enc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (dec_blocks): ModuleList(\n",
       "    (0-11): 12 x DecoderBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): RoPE2D()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (projq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): RoPE2D()\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dec_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (dec_blocks2): ModuleList(\n",
       "    (0-11): 12 x DecoderBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): RoPE2D()\n",
       "      )\n",
       "      (cross_attn): CrossAttention(\n",
       "        (projq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (projv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (rope): RoPE2D()\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm_y): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (downstream_head1): PixelwiseTaskWithDPT(\n",
       "    (dpt): DPTOutputAdapter_fix(\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer_rn): ModuleList(\n",
       "          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (refinenet1): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Interpolate()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act_postprocess): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downstream_head2): PixelwiseTaskWithDPT(\n",
       "    (dpt): DPTOutputAdapter_fix(\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer_rn): ModuleList(\n",
       "          (0): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (refinenet1): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock_custom(\n",
       "          (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit_custom(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Interpolate()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act_postprocess): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchprofile\n",
    "from torchinfo import summary\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.datasets.co3d import Co3d\n",
    "from dust3r.datasets import get_data_loader \n",
    "def build_dataset(dataset, batch_size, num_workers, test=False):\n",
    "    split = ['Train', 'Test'][test]\n",
    "    print(f'Building {split} Data loader for dataset: ', dataset)\n",
    "    loader = get_data_loader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_mem=True,\n",
    "                             shuffle=not (test),\n",
    "                             drop_last=not (test))\n",
    "\n",
    "    print(f\"{split} dataset length: \", len(loader))\n",
    "    return loader\n",
    "args = {}\n",
    "inf = float('inf')\n",
    "dataset = Co3d(split='train', ROOT=\"data/co3d_subset_processed\", resolution=512, aug_crop=16)\n",
    "data_loader_train = build_dataset(dataset,1, 1, test=False)\n",
    "view1, view2 = next(iter(data_loader_train))\n",
    "args = {\n",
    "    'pos_embed': 'RoPE100',\n",
    "    'img_size': (512, 512),\n",
    "    'head_type': 'dpt',\n",
    "    'output_mode': 'pts3d',\n",
    "    'depth_mode': ('exp', -inf, inf),\n",
    "    'conf_mode': ('exp', 1, inf),\n",
    "    'enc_embed_dim': 1024,\n",
    "    'enc_depth': 24,\n",
    "    'enc_num_heads': 16,\n",
    "    'dec_embed_dim': 768,\n",
    "    'dec_depth': 12,\n",
    "    'dec_num_heads': 12\n",
    "\n",
    "}\n",
    "model = AsymmetricCroCo3DStereo(**args)\n",
    "model.__repr__\n",
    "#summary(model,col_names=(\"input_size\",\"output_size\",\"num_params\",),row_settings=('var_names','depth'), depth=10,input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++ENCODER++++++++++++++++++\n",
      "feat1 torch.Size([1, 1024, 1024])\n",
      "feat2 torch.Size([1, 1024, 1024])\n",
      "++++++++++++++++++DECODER++++++++++++++++++\n",
      "0 _f1 torch.Size([1, 1024, 1024])\n",
      "0 _f2 torch.Size([1, 1024, 1024])\n",
      "decoder_embed f1 torch.Size([1, 1024, 768])\n",
      "decoder_embed f2 torch.Size([1, 1024, 768])\n",
      "1 _f1 torch.Size([1, 1024, 768])\n",
      "1 _f2 torch.Size([1, 1024, 768])\n",
      "2 _f1 torch.Size([1, 1024, 768])\n",
      "2 _f2 torch.Size([1, 1024, 768])\n",
      "3 _f1 torch.Size([1, 1024, 768])\n",
      "3 _f2 torch.Size([1, 1024, 768])\n",
      "4 _f1 torch.Size([1, 1024, 768])\n",
      "4 _f2 torch.Size([1, 1024, 768])\n",
      "5 _f1 torch.Size([1, 1024, 768])\n",
      "5 _f2 torch.Size([1, 1024, 768])\n",
      "6 _f1 torch.Size([1, 1024, 768])\n",
      "6 _f2 torch.Size([1, 1024, 768])\n",
      "7 _f1 torch.Size([1, 1024, 768])\n",
      "7 _f2 torch.Size([1, 1024, 768])\n",
      "8 _f1 torch.Size([1, 1024, 768])\n",
      "8 _f2 torch.Size([1, 1024, 768])\n",
      "9 _f1 torch.Size([1, 1024, 768])\n",
      "9 _f2 torch.Size([1, 1024, 768])\n",
      "10 _f1 torch.Size([1, 1024, 768])\n",
      "10 _f2 torch.Size([1, 1024, 768])\n",
      "11 _f1 torch.Size([1, 1024, 768])\n",
      "11 _f2 torch.Size([1, 1024, 768])\n",
      "12 _f1 torch.Size([1, 1024, 768])\n",
      "12 _f2 torch.Size([1, 1024, 768])\n",
      "final_output before 14\n",
      "final_output after 13\n",
      "+++++++++++++++++++DPThead+++++++++++++++++++\n",
      "adapt_tokens0 torch.Size([1, 1024, 1024])\n",
      "adapt_tokens1 torch.Size([1, 1024, 768])\n",
      "adapt_tokens2 torch.Size([1, 1024, 768])\n",
      "adapt_tokens3 torch.Size([1, 1024, 768])\n",
      "++++++++++++++++++++++++++++++++\n",
      "reshape0 torch.Size([1, 1024, 32, 32])\n",
      "reshape1 torch.Size([1, 768, 32, 32])\n",
      "reshape2 torch.Size([1, 768, 32, 32])\n",
      "reshape3 torch.Size([1, 768, 32, 32])\n",
      "++++++++++++++++++++++++++++++++\n",
      "act_postprocess0 torch.Size([1, 96, 128, 128])\n",
      "act_postprocess1 torch.Size([1, 192, 64, 64])\n",
      "act_postprocess2 torch.Size([1, 384, 32, 32])\n",
      "act_postprocess3 torch.Size([1, 768, 16, 16])\n",
      "++++++++++++++++++++++++++++++++\n",
      "rn0 torch.Size([1, 256, 128, 128])\n",
      "rn1 torch.Size([1, 256, 64, 64])\n",
      "rn2 torch.Size([1, 256, 32, 32])\n",
      "rn3 torch.Size([1, 256, 16, 16])\n",
      "++++++++++++++++++++++++++++++++\n",
      "layers3 torch.Size([1, 256, 16, 16])\n",
      "path4 torch.Size([1, 256, 32, 32]) layer2 torch.Size([1, 256, 32, 32])\n",
      "path3 torch.Size([1, 256, 64, 64]) layer1: torch.Size([1, 256, 64, 64])\n",
      "path2 torch.Size([1, 256, 128, 128]) layer0: torch.Size([1, 256, 128, 128])\n",
      "path1 torch.Size([1, 256, 256, 256])\n",
      "+++++++++++++++++++DPThead+++++++++++++++++++\n",
      "adapt_tokens0 torch.Size([1, 1024, 1024])\n",
      "adapt_tokens1 torch.Size([1, 1024, 768])\n",
      "adapt_tokens2 torch.Size([1, 1024, 768])\n",
      "adapt_tokens3 torch.Size([1, 1024, 768])\n",
      "++++++++++++++++++++++++++++++++\n",
      "reshape0 torch.Size([1, 1024, 32, 32])\n",
      "reshape1 torch.Size([1, 768, 32, 32])\n",
      "reshape2 torch.Size([1, 768, 32, 32])\n",
      "reshape3 torch.Size([1, 768, 32, 32])\n",
      "++++++++++++++++++++++++++++++++\n",
      "act_postprocess0 torch.Size([1, 96, 128, 128])\n",
      "act_postprocess1 torch.Size([1, 192, 64, 64])\n",
      "act_postprocess2 torch.Size([1, 384, 32, 32])\n",
      "act_postprocess3 torch.Size([1, 768, 16, 16])\n",
      "++++++++++++++++++++++++++++++++\n",
      "rn0 torch.Size([1, 256, 128, 128])\n",
      "rn1 torch.Size([1, 256, 64, 64])\n",
      "rn2 torch.Size([1, 256, 32, 32])\n",
      "rn3 torch.Size([1, 256, 16, 16])\n",
      "++++++++++++++++++++++++++++++++\n",
      "layers3 torch.Size([1, 256, 16, 16])\n",
      "path4 torch.Size([1, 256, 32, 32]) layer2 torch.Size([1, 256, 32, 32])\n",
      "path3 torch.Size([1, 256, 64, 64]) layer1: torch.Size([1, 256, 64, 64])\n",
      "path2 torch.Size([1, 256, 128, 128]) layer0: torch.Size([1, 256, 128, 128])\n",
      "path1 torch.Size([1, 256, 256, 256])\n",
      "torch.Size([1, 512, 512])\n",
      "torch.Size([1, 512, 512, 3])\n"
     ]
    }
   ],
   "source": [
    "pred1, pred2 = model(view1, view2)\n",
    "print(pred1['conf'].shape)\n",
    "print(pred1['pts3d'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers0 torch.Size([1, 1024, 768])\n",
      "layers1 torch.Size([1, 1024, 768])\n",
      "layers2 torch.Size([1, 1024, 768])\n",
      "layers3 torch.Size([1, 1024, 768])\n",
      "++++++++++++++++++++++++++++++++\n",
      "adapt_tokens0 torch.Size([1, 1024, 768])\n",
      "adapt_tokens1 torch.Size([1, 1024, 768])\n",
      "adapt_tokens2 torch.Size([1, 1024, 768])\n",
      "adapt_tokens3 torch.Size([1, 1024, 768])\n",
      "++++++++++++++++++++++++++++++++\n",
      "reshape0 torch.Size([1, 768, 32, 32])\n",
      "reshape1 torch.Size([1, 768, 32, 32])\n",
      "reshape2 torch.Size([1, 768, 32, 32])\n",
      "reshape3 torch.Size([1, 768, 32, 32])\n",
      "++++++++++++++++++++++++++++++++\n",
      "act_postprocess0 torch.Size([1, 96, 128, 128])\n",
      "act_postprocess1 torch.Size([1, 192, 64, 64])\n",
      "act_postprocess2 torch.Size([1, 384, 32, 32])\n",
      "act_postprocess3 torch.Size([1, 768, 16, 16])\n",
      "++++++++++++++++++++++++++++++++\n",
      "rn0 torch.Size([1, 196, 128, 128])\n",
      "rn1 torch.Size([1, 196, 64, 64])\n",
      "rn2 torch.Size([1, 196, 32, 32])\n",
      "rn3 torch.Size([1, 196, 16, 16])\n",
      "++++++++++++++++++++++++++++++++\n",
      "layers3 torch.Size([1, 196, 16, 16])\n",
      "path4 torch.Size([1, 196, 32, 32]) layer2 torch.Size([1, 196, 32, 32])\n",
      "path3 torch.Size([1, 196, 64, 64]) layer1: torch.Size([1, 196, 64, 64])\n",
      "path2 torch.Size([1, 196, 128, 128]) layer0: torch.Size([1, 196, 128, 128])\n",
      "path1 torch.Size([1, 196, 256, 256])\n",
      "output torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dust3r.dpt_custom import DPTOutputAdapter\n",
    "\n",
    "args = {\n",
    "    'num_channels': 1,\n",
    "    'stride_level': 1,\n",
    "    'patch_size': 16,\n",
    "    'main_tasks': ('rgb',),\n",
    "    'hooks': [2, 5, 8, 11],\n",
    "    'layer_dims': [96, 192, 384, 768],\n",
    "    'feature_dim': 196,\n",
    "    'last_dim': 32,\n",
    "    'use_bn': False,\n",
    "    'dim_tokens_enc': 768,\n",
    "    'head_type': 'regression',\n",
    "    'output_width_ratio': 1\n",
    "}\n",
    "model = DPTOutputAdapter(**args)\n",
    "model.init()\n",
    "#print(model.__repr__())\n",
    "# Create dummy input\n",
    "encoder_tokens = [(torch.ones(1,1024,768)+i) for i in range(13)]\n",
    "\n",
    "image_size = (512, 512)\n",
    "\n",
    "# Forward pass\n",
    "output = model(encoder_tokens, image_size)\n",
    "\n",
    "# Print the output\n",
    "print(\"output\",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Custom_Head(nn.Module):\n",
    "    def __init__(self,\n",
    "                 last_dim,\n",
    "                 use_bn,\n",
    "                 dim_tokens_enc,\n",
    "                 output_width_ratio,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]])\n",
      "tensor([[[3., 3., 3.,  ..., 3., 3., 3.],\n",
      "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "         ...,\n",
      "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
      "         [3., 3., 3.,  ..., 3., 3., 3.]]])\n",
      "tensor([[[4., 4., 4.,  ..., 4., 4., 4.],\n",
      "         [4., 4., 4.,  ..., 4., 4., 4.],\n",
      "         [4., 4., 4.,  ..., 4., 4., 4.],\n",
      "         ...,\n",
      "         [4., 4., 4.,  ..., 4., 4., 4.],\n",
      "         [4., 4., 4.,  ..., 4., 4., 4.],\n",
      "         [4., 4., 4.,  ..., 4., 4., 4.]]])\n",
      "tensor([[[5., 5., 5.,  ..., 5., 5., 5.],\n",
      "         [5., 5., 5.,  ..., 5., 5., 5.],\n",
      "         [5., 5., 5.,  ..., 5., 5., 5.],\n",
      "         ...,\n",
      "         [5., 5., 5.,  ..., 5., 5., 5.],\n",
      "         [5., 5., 5.,  ..., 5., 5., 5.],\n",
      "         [5., 5., 5.,  ..., 5., 5., 5.]]])\n",
      "tensor([[[6., 6., 6.,  ..., 6., 6., 6.],\n",
      "         [6., 6., 6.,  ..., 6., 6., 6.],\n",
      "         [6., 6., 6.,  ..., 6., 6., 6.],\n",
      "         ...,\n",
      "         [6., 6., 6.,  ..., 6., 6., 6.],\n",
      "         [6., 6., 6.,  ..., 6., 6., 6.],\n",
      "         [6., 6., 6.,  ..., 6., 6., 6.]]])\n",
      "tensor([[[7., 7., 7.,  ..., 7., 7., 7.],\n",
      "         [7., 7., 7.,  ..., 7., 7., 7.],\n",
      "         [7., 7., 7.,  ..., 7., 7., 7.],\n",
      "         ...,\n",
      "         [7., 7., 7.,  ..., 7., 7., 7.],\n",
      "         [7., 7., 7.,  ..., 7., 7., 7.],\n",
      "         [7., 7., 7.,  ..., 7., 7., 7.]]])\n",
      "tensor([[[8., 8., 8.,  ..., 8., 8., 8.],\n",
      "         [8., 8., 8.,  ..., 8., 8., 8.],\n",
      "         [8., 8., 8.,  ..., 8., 8., 8.],\n",
      "         ...,\n",
      "         [8., 8., 8.,  ..., 8., 8., 8.],\n",
      "         [8., 8., 8.,  ..., 8., 8., 8.],\n",
      "         [8., 8., 8.,  ..., 8., 8., 8.]]])\n",
      "tensor([[[9., 9., 9.,  ..., 9., 9., 9.],\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "         ...,\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "         [9., 9., 9.,  ..., 9., 9., 9.]]])\n",
      "tensor([[[10., 10., 10.,  ..., 10., 10., 10.],\n",
      "         [10., 10., 10.,  ..., 10., 10., 10.],\n",
      "         [10., 10., 10.,  ..., 10., 10., 10.],\n",
      "         ...,\n",
      "         [10., 10., 10.,  ..., 10., 10., 10.],\n",
      "         [10., 10., 10.,  ..., 10., 10., 10.],\n",
      "         [10., 10., 10.,  ..., 10., 10., 10.]]])\n",
      "tensor([[[11., 11., 11.,  ..., 11., 11., 11.],\n",
      "         [11., 11., 11.,  ..., 11., 11., 11.],\n",
      "         [11., 11., 11.,  ..., 11., 11., 11.],\n",
      "         ...,\n",
      "         [11., 11., 11.,  ..., 11., 11., 11.],\n",
      "         [11., 11., 11.,  ..., 11., 11., 11.],\n",
      "         [11., 11., 11.,  ..., 11., 11., 11.]]])\n",
      "tensor([[[12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         ...,\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]]])\n",
      "tensor([[[13., 13., 13.,  ..., 13., 13., 13.],\n",
      "         [13., 13., 13.,  ..., 13., 13., 13.],\n",
      "         [13., 13., 13.,  ..., 13., 13., 13.],\n",
      "         ...,\n",
      "         [13., 13., 13.,  ..., 13., 13., 13.],\n",
      "         [13., 13., 13.,  ..., 13., 13., 13.],\n",
      "         [13., 13., 13.,  ..., 13., 13., 13.]]])\n"
     ]
    }
   ],
   "source": [
    "encoder_tokens = [(torch.ones(1,1024,768)+i) for i in range(13)]\n",
    "for i in range(13):\n",
    "    print(encoder_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d parameter sizes:\n",
      "weight: torch.Size([768, 768, 1])\n",
      "bias: torch.Size([768])\n",
      "\n",
      "linear parameter sizes:\n",
      "weight: torch.Size([768, 768])\n",
      "bias: torch.Size([768])\n",
      "The difference in parameter count between conv1d and linear is: 0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import torch\n",
    "\n",
    "input_size = 768\n",
    "output_size = 768\n",
    "input_tensor = torch.randn(1, 768, 768)\n",
    "\n",
    "conv1d = nn.Conv1d(768, 768,1)\n",
    "linear = nn.Linear(768, 768)\n",
    "print(\"conv1d parameter sizes:\")\n",
    "for name, param in conv1d.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")\n",
    "\n",
    "print(\"\\nlinear parameter sizes:\")\n",
    "for name, param in linear.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")\n",
    "\n",
    "conv1d_params = sum(p.numel() for p in conv1d.parameters())\n",
    "linear_params = sum(p.numel() for p in linear.parameters())\n",
    "\n",
    "param_diff = conv1d_params - linear_params\n",
    "print(f\"The difference in parameter count between conv1d and linear is: {param_diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
